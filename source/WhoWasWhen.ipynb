{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc1229a",
   "metadata": {},
   "source": [
    "# WhoWasWhen generator\n",
    "This script is to create the database for the WhoWasWhen workflow, starting from a google spreadsheet\n",
    "it can be (it used to be actually) simplified to use a tab-delimited local file, but since my main file is a google sheet I didn't want to export and save all the time\n",
    "the script creates a JSON object with keys = year and values a nested dict with all the rulers\n",
    "Sunny ‚òÄÔ∏è   üå°Ô∏è+76¬∞F (feels +76¬∞F, 32%) üå¨Ô∏è‚Üò6mph üåò&m Sat Jun  1 11:17:32 2024\n",
    "W22Q2 ‚Äì 153 ‚û°Ô∏è 212 ‚Äì 21 ‚ùáÔ∏è 343"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440404f",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472b55dc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# home: use the 3.10.9 Python environment\n",
    "import pickle\n",
    "import json\n",
    "import sqlite3\n",
    "import gspread\n",
    "import gspread.exceptions\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2 import service_account\n",
    "from time import time\n",
    "from config import KEYFILE, log, GSHEET_URL, MY_PERIOD_SHEET, MY_RULERS_SHEET, MY_DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826a8a7",
   "metadata": {},
   "source": [
    "## GetSheet\n",
    "A function to retrieve tables from a google sheet and convert them into a nested dictionary\n",
    "(The conversion might not be needed in the new version using sqlite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcb689f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def getSheet(keyfile,mysheetURL, myPeriodSheet, myColumns):\n",
    "    def list_to_nested_dict(all_values, selected_columns):\n",
    "        headers = all_values[0]  # The first row is the header\n",
    "        indices = [headers.index(col) for col in selected_columns if col in headers]  # Indices of selected columns\n",
    "        data_dict = {}\n",
    "        \n",
    "        for row in all_values[1:]:\n",
    "            # Create a dictionary for each row with selected columns only\n",
    "            row_dict = {headers[i]: row[i] for i in indices}\n",
    "            # Use the first column as the key for the main dictionary\n",
    "            data_dict[row[0]] = row_dict\n",
    "        \n",
    "        return data_dict\n",
    "\n",
    "    scopes = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    'https://www.googleapis.com/auth/drive'\n",
    "    ]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(keyfile, scopes) \n",
    "    file = gspread.authorize(credentials) # authenticate the JSON key with gspread\n",
    "    \n",
    "    # opening the file\n",
    "    try:\n",
    "        sheet = file.open_by_url(mysheetURL)\n",
    "    \n",
    "    except gspread.exceptions.NoValidUrlKeyFound as e:\n",
    "        log (\"URL not valid\")\n",
    "    except ValueError as e:\n",
    "        log (\"ValueError\")\n",
    "    except IOError as e:\n",
    "    # Handle input/output errors\n",
    "        log (\"IOError\")\n",
    "    except Exception as e:\n",
    "    # Catch any remaining errors\n",
    "        log (\"Exception\")\n",
    "\n",
    "    \n",
    "    # fetching the worksheet\n",
    "    try:\n",
    "        worksheet = sheet.worksheet(myPeriodSheet) \n",
    "    \n",
    "    except gspread.exceptions.WorksheetNotFound as e:\n",
    "        log (\"Sheet Not Found\")\n",
    "\n",
    "\n",
    "    # Get all values from the worksheet\n",
    "    all_values = worksheet.get_all_values()\n",
    "\n",
    "   \n",
    "    # convertint into a dictionary with key = counter and value = all values\n",
    "    data_dict = list_to_nested_dict(all_values, myColumns)\n",
    "    \n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e89fd0",
   "metadata": {},
   "source": [
    "## Populating tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6900c5",
   "metadata": {},
   "source": [
    "### Parse period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1209a02d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_period(years):\n",
    "# Function to expand the years field (by ChatGPT)\n",
    "    if '-' in years:\n",
    "        start_year, end_year = years.split('-')\n",
    "        \n",
    "        # Convert start year\n",
    "        if 'BC' in start_year:\n",
    "            start_year = -int(start_year.replace('BC', ''))\n",
    "        else:\n",
    "            start_year = int(start_year)\n",
    "\n",
    "        # Convert end year\n",
    "        if 'BC' in end_year:\n",
    "            end_year = -int(end_year.replace('BC', ''))\n",
    "        else:\n",
    "            if 'AD' in end_year:\n",
    "                end_year = int(end_year.replace('AD', ''))\n",
    "            elif len(end_year) <= 2:\n",
    "                # Handle short form end year like '95' in '1981-95'\n",
    "                end_year = int(f\"{str(start_year)[:-len(end_year)]}{end_year}\")\n",
    "            else:\n",
    "                end_year = int(end_year)\n",
    "        \n",
    "\n",
    "        \n",
    "    else:\n",
    "        if 'BC' in years:\n",
    "            start_year = end_year = -int(years.replace('BC', ''))\n",
    "        elif 'AD' in years:\n",
    "            start_year = end_year = -int(years.replace('AD', ''))\n",
    "        else:\n",
    "            start_year = end_year = int(years)\n",
    "    return start_year, end_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518dd6b",
   "metadata": {},
   "source": [
    "### populateTables function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712128d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateTables(myData, db_name):\n",
    "\n",
    "    def creatingTables():\n",
    "        \n",
    "        # Drop tables if they exist\n",
    "        cursor.execute('DROP TABLE IF EXISTS titles;')\n",
    "        cursor.execute('DROP TABLE IF EXISTS years;')\n",
    "        cursor.execute('DROP TABLE IF EXISTS byPeriod;')\n",
    "        cursor.execute('DROP TABLE IF EXISTS byYear;')\n",
    "\n",
    "\n",
    "        # Create the tables\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS titles (\n",
    "            titleID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT UNIQUE,\n",
    "            maxCount INTEGER,\n",
    "            titlePlural TEXT\n",
    "        );\n",
    "        ''')\n",
    "\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS years (\n",
    "            yearID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            year INTEGER UNIQUE\n",
    "        );\n",
    "        ''')\n",
    "\n",
    "\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS byPeriod (\n",
    "            periodID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            rulerID INTEGER,\n",
    "            titleID INTEGER,\n",
    "            progrTitle INTEGER,\n",
    "            period TEXT,\n",
    "            startYear INTEGER,\n",
    "            endYear INTEGER,\n",
    "            notes TEXT,\n",
    "                FOREIGN KEY (rulerID) REFERENCES rulers (rulerID),\n",
    "                FOREIGN KEY (titleID) REFERENCES titles (titleID)\n",
    "            );\n",
    "        ''')\n",
    "\n",
    "\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS byYear (\n",
    "            yearID INTEGER,\n",
    "            periodID INTEGER,\n",
    "                FOREIGN KEY (yearID) REFERENCES years (yearID),\n",
    "                FOREIGN KEY (periodID) REFERENCES byPeriod (periodID),\n",
    "                PRIMARY KEY (yearID, periodID)\n",
    "        );\n",
    "        ''')\n",
    "    def fetchTitleID(title):\n",
    "        # Function to get the titleID for a given title\n",
    "        cursor.execute('''\n",
    "            SELECT titleID FROM titles WHERE title = ?\n",
    "        ''', (title,))\n",
    "        title_id = cursor.fetchone()\n",
    "        return title_id[0] if title_id else None\n",
    "\n",
    "    def insert_title(title):\n",
    "    # Function to insert a unique title and get its titleID\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO titles (title)\n",
    "            VALUES (?)\n",
    "            ''', (title,))\n",
    "        conn.commit()\n",
    "        \n",
    "\n",
    "        # Retrieve the titleID for the given title\n",
    "        title_id = fetchTitleID(title)\n",
    "        \n",
    "        return title_id if title_id else None\n",
    "\n",
    "    def populate_byPeriod (rulerID, titleID, progrTitle, period, startYear, endYear,notes):\n",
    "\n",
    "        # Insert each period in the byPeriod table and get the periodID\n",
    "        cursor.execute('''\n",
    "            INSERT INTO byPeriod (rulerID, titleID, progrTitle, period, startYear, endYear,notes)\n",
    "            VALUES (?, ?, ?, ?, ?, ?,?)\n",
    "            ''', (rulerID, titleID, progrTitle, period, startYear, endYear,notes))\n",
    "        conn.commit()    \n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def insert_year(year):\n",
    "    # Function to insert unique year and get its yearID\n",
    "        cursor.execute('''\n",
    "        INSERT OR IGNORE INTO years (year)\n",
    "        VALUES (?)\n",
    "        ''', (year,))\n",
    "        conn.commit()\n",
    "        # Retrieve the yearID for the given year\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT yearID FROM years WHERE year = ?\n",
    "        ''', (year,))\n",
    "        year_id = cursor.fetchone()\n",
    "        \n",
    "        # Return the yearID\n",
    "        return year_id[0] if year_id else None\n",
    "\n",
    "    def populateByYear (periodID, startYear, endYear):\n",
    "        # Function to insert ruler title relationships for each year\n",
    "\n",
    "        for year in range(startYear, endYear + 1):\n",
    "\n",
    "            # Insert each year into the years table and get the yearID\n",
    "            yearID = insert_year(year)\n",
    "\n",
    "\n",
    "            # Insert periodID and yearID into the byYear table\n",
    "            cursor.execute('''\n",
    "            INSERT OR IGNORE INTO byYear (periodID, yearID)\n",
    "            VALUES (?, ?)\n",
    "            ''', (periodID, yearID))\n",
    "        conn.commit()\n",
    "\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create tables\n",
    "    creatingTables()\n",
    "    \n",
    "    # processing the dictionary\n",
    "    titleCheck = {}\n",
    "    for key,row in myData.items():\n",
    "        title = row['Title']\n",
    "        rulerID = row['RulerID']\n",
    "        period = row['Period'].strip()  # e.g. \"1509-1547\"\n",
    "        notes = row['Notes']\n",
    "        startYear, endYear = parse_period(period)\n",
    "\n",
    "        if title not in titleCheck:\n",
    "            titleCheck[title] = 1\n",
    "            titleID = insert_title(title)\n",
    "            # Insert the title into the titles table and get its titleID\n",
    "        else:\n",
    "            titleCheck[title] += 1\n",
    "            titleID = fetchTitleID(title)\n",
    "            \n",
    "        \n",
    "        periodID = populate_byPeriod(rulerID, titleID, titleCheck[title], period, startYear, endYear, notes)        \n",
    "        \n",
    "        # Insert year and period relationships into the byYear table\n",
    "        populateByYear (periodID, startYear, endYear)\n",
    "        \n",
    "    for myTitle in titleCheck:\n",
    "        cursor.execute('''\n",
    "            UPDATE titles\n",
    "            SET maxCount = ?\n",
    "            WHERE title = ?\n",
    "        ''', (titleCheck[myTitle], myTitle))\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "    # adding plurals\n",
    "    myPlurals = {'Pope': 'Popes', \n",
    "                 'English Monarch': 'English Monarchs', \n",
    "                 'US president': 'US presidents', \n",
    "                 'British Prime Minister': 'British Prime Ministers', \n",
    "                 'French Monarch': 'French Monarchs', \n",
    "                 'King of the West Franks': 'Kings of the West Franks', \n",
    "                 'King of the East Franks': 'Kings of the East Franks', \n",
    "                 'King of the Franks': 'Kings of the Franks', \n",
    "                 'King of France': 'Kings of France', \n",
    "                 'French Government': 'French Governments', \n",
    "                 'French Emperor': 'French Emperors', \n",
    "                 'French President': 'French Presidents', \n",
    "                 'Byzantine Emperor': 'Byzantine Emperors', \n",
    "                 'Emperor of China': 'Emperors of China', \n",
    "                 'Holy Roman Emperor': 'Holy Roman Emperors', \n",
    "                 'Emperor of the Carolingian Empire': 'Emperors of the Carolingian Empire', \n",
    "                 'Roman Emperor': 'Roman Emperors', \n",
    "                 'Roman Emperor (East)': 'Roman Emperors (East)', \n",
    "                 'Roman Emperor (West)': 'Roman Emperors (West)', \n",
    "                 'Russian Emperor': 'Russian Emperors', \n",
    "                 'Prince of Moscow': 'Princes of Moscow', \n",
    "                 'Tsar of Russia': 'Tsars of Russia', \n",
    "                 'Chairman of the Communist Party of the Soviet Union': 'Chairmen of the Communist Party of the Soviet Union', \n",
    "                 'Russian President': 'Russian Presidents',\n",
    "                 'Antipope': 'Antipopes',\n",
    "                 'Scottish Monarch': 'Scottish Monarchs',\n",
    "                 'Neapolitan ruler': 'Neapolitan rulers',\n",
    "                 'Spanish Monarch': 'Spanish Monarchs'}\n",
    "    \n",
    "    for myTitle in myPlurals:\n",
    "        cursor.execute('''\n",
    "            UPDATE titles\n",
    "            SET titlePlural = ?\n",
    "            WHERE title = ?\n",
    "        ''', (myPlurals[myTitle], myTitle))\n",
    "    conn.commit()\n",
    "\n",
    "    log(\"Titles and junction table successfully exported to SQLite database\")\n",
    "    \n",
    "    # Close the connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48bff3",
   "metadata": {},
   "source": [
    "### Populate Rulers\n",
    "Creating the `rulers` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e94d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateRulers(myData, db_name):\n",
    "    # Connect to SQLite database (or create it if it doesn't exist)\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('DROP TABLE IF EXISTS rulers;')\n",
    "    # Create the rulers table\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS rulers (\n",
    "            rulerID INTEGER PRIMARY KEY,\n",
    "            name TEXT,\n",
    "            personal_name TEXT,\n",
    "            epithet TEXT,\n",
    "            wikipedia TEXT,\n",
    "            notes TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert data into the rulers table\n",
    "    for key, row in myData.items():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR IGNORE INTO rulers (\n",
    "                rulerID, name, personal_name, epithet, wikipedia, notes\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            int(row['RulerID']),\n",
    "            row['Name'],\n",
    "            row['Personal Name'],\n",
    "            row['Epithet'],\n",
    "            row['Wikipedia'],\n",
    "            row['Notes']\n",
    "        ))\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    log(f\"Rulers table successfully exported to SQLite database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf9f3b",
   "metadata": {},
   "source": [
    "# üìç Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9cc1856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "script duration: 1.158 seconds\n",
      "Rulers table successfully exported to SQLite database\n",
      "Titles and junction table successfully exported to SQLite database\n",
      "Done üëçÔ∏è\n"
     ]
    }
   ],
   "source": [
    "main_start_time = time()\n",
    "# first, I need to get the 2 spreadheets from the google sheet: 1) rulers and 2) periods\n",
    "\n",
    "# get rulers\n",
    "allRulers = getSheet(KEYFILE, GSHEET_URL, MY_RULERS_SHEET, [\"RulerID\",\"Name\",\"Personal Name\",\"Wikipedia\", \"Epithet\",\"Personal Name\", \"Notes\"])\n",
    "\n",
    "# get periods\n",
    "selected_columns = [\"Title\", \"RulerID\", \"Period\", \"Notes\"] #columns to be fetched from the gsheet\n",
    "allValues = getSheet(KEYFILE, GSHEET_URL, MY_PERIOD_SHEET, selected_columns)\n",
    "\n",
    "main_timeElapsed = time() - main_start_time\n",
    "log(f\"\\nscript duration: {round (main_timeElapsed,3)} seconds\")\n",
    "    \n",
    "\n",
    "# exportRulersYears(allValues,allRulers)\n",
    "# createIconDict()\n",
    "\n",
    "# second, I create the sqlite tables\n",
    "populateRulers(allRulers, MY_DB)\n",
    "\n",
    "populateTables(allValues, MY_DB)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# result= {\"items\": [{\n",
    "#     \"title\": \"Done!\" ,\n",
    "#     \"subtitle\": \"ready to use WhoWhasWhen now üëçÔ∏è\",\n",
    "#     \"arg\": \"\",\n",
    "#     \"icon\": {\n",
    "\n",
    "#             \"path\": \"icons/done.png\"\n",
    "#         }\n",
    "#     }]}\n",
    "# print (json.dumps(result))\n",
    "log(\"Done üëçÔ∏è\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ebc35",
   "metadata": {},
   "source": [
    "# Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93855da",
   "metadata": {},
   "source": [
    "## Extracting tables from a wikipedia page  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0c502e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables have been saved into a single CSV file: 'roman_consuls_combined.csv'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script from chatGPT will extract and parse multiple tables from a single Wikipedia page and export them into a csv file and dictionary. Used to output the roman consuls from wikipedia\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Roman_consuls\"\n",
    "\n",
    "# Send a GET request to the Wikipedia page\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all tables on the page\n",
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "# Initialize a list to store all tables\n",
    "all_tables = []\n",
    "\n",
    "# Function to ensure unique column names\n",
    "def make_unique_columns(columns):\n",
    "    seen = {}\n",
    "    unique_columns = []\n",
    "    for col in columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            unique_columns.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            unique_columns.append(col)\n",
    "    return unique_columns\n",
    "\n",
    "# Loop through all tables and process them\n",
    "for i, table in enumerate(tables):\n",
    "    # Extract table headers\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "    headers = make_unique_columns(headers)  # Ensure unique headers\n",
    "    \n",
    "    # Prepare data for rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr'):\n",
    "        row_data = []\n",
    "        row_links = []\n",
    "        for cell in row.find_all(['td', 'th']):\n",
    "            # Get text in the cell\n",
    "            cell_text = cell.text.strip()\n",
    "            # Get hyperlink if present\n",
    "            link = cell.find('a')\n",
    "            cell_link = f\"https://en.wikipedia.org{link['href']}\" if link and link.has_attr('href') else None\n",
    "            # Append text and link\n",
    "            row_data.append(cell_text)\n",
    "            row_links.append(cell_link)\n",
    "        # Append row only if there's data\n",
    "        if row_data:\n",
    "            rows.append((row_data, row_links))\n",
    "    \n",
    "    # Prepare columns for text and links\n",
    "    text_data = [row[0] for row in rows]\n",
    "    link_data = [row[1] for row in rows]\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df = pd.DataFrame(text_data, columns=headers if headers else [f\"Column_{j+1}\" for j in range(len(text_data[0]))])\n",
    "    link_headers = [f\"{col}_link\" for col in df.columns]\n",
    "    link_df = pd.DataFrame(link_data, columns=link_headers)\n",
    "    \n",
    "    # Combine text and link DataFrames\n",
    "    combined_df = pd.concat([df, link_df], axis=1)\n",
    "    combined_df['Source_Table'] = f\"Table_{i+1}\"  # Add a column to identify the source table\n",
    "    \n",
    "    # Add the combined DataFrame to the list\n",
    "    all_tables.append(combined_df)\n",
    "\n",
    "# Concatenate all tables into one DataFrame\n",
    "final_df = pd.concat(all_tables, ignore_index=True)  # Ensure unique indexing\n",
    "\n",
    "# Save the concatenated DataFrame to a single CSV file\n",
    "final_df.to_csv(\"roman_consuls_combined.csv\", index=False)\n",
    "\n",
    "print(\"All tables have been saved into a single CSV file: 'roman_consuls_combined.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44832b5",
   "metadata": {},
   "source": [
    "# Older code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OLDER CODE TO DELETE\n",
    "def exportRulers (myData):\n",
    "    # Initialize the dictionary\n",
    "    rulers = {}\n",
    "    for key,row in myData.items():\n",
    "        \n",
    "        rulers [row['RulerID']] = {\n",
    "            \"name\": row['Name'],\n",
    "            \"personal name\": row['Personal Name'],\n",
    "            \"epithet\": row['Epithet'],\n",
    "            \"wikipedia\": row['Wikipedia'],\n",
    "            \"title\": [],\n",
    "            \"notes\": row['Notes']\n",
    "        }\n",
    "    \n",
    "    # # Export the resulting dictionary to a JSON file\n",
    "    return rulers\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportRulersYears (myData,myRulers):\n",
    "    # Initialize the dictionary\n",
    "    rulerYears_dict = {}\n",
    "    rulers_dict = {}\n",
    "    myCounter = 0\n",
    "    for key,row in myData.items():\n",
    "        \n",
    "        ruler_type = row['Ruler']\n",
    "        myCounter += 1\n",
    "       \n",
    "        name = row['Name']\n",
    "        years_field = row['Year'].strip()\n",
    "        personalName = row['Personal Name']\n",
    "        years = expand_years(years_field)\n",
    "        year_max = max(years)\n",
    "        year_min = min(years)\n",
    "\n",
    "        if ruler_type in rulers_dict:\n",
    "            rulers_dict[ruler_type].append({'progr': myCounter,\n",
    "                                            'name': row['Name'],\n",
    "                                            'personal name': row['Personal Name'],\n",
    "                                            'period': row['Year'],\n",
    "                                            'startYear': year_min,\n",
    "                                            'endYear': year_max,\n",
    "                                            'rulerID': row['RulerID']\n",
    "                                            })\n",
    "        else:\n",
    "            # initialize ruler type\n",
    "            myCounter = 1\n",
    "            rulers_dict[ruler_type] = [{'progr': myCounter,\n",
    "                                        'name': row['Name'],\n",
    "                                        'personal name': row['Personal Name'],\n",
    "                                        'period': row['Year'],\n",
    "                                        'startYear': year_min,\n",
    "                                        'endYear': year_max,\n",
    "                                        'rulerID': row['RulerID']\n",
    "\n",
    "                                        }]\n",
    "        if row['RulerID'] in myRulers.keys():\n",
    "            if row['Ruler'] not in myRulers[row['RulerID']]['title']:\n",
    "                myRulers[row['RulerID']]['title'].append ((row['Ruler'],row['Year']))\n",
    "            \n",
    "        \n",
    "        for year in years:\n",
    "            year_str = str(year)\n",
    "            if year_str not in rulerYears_dict:\n",
    "                rulerYears_dict[year_str] = {\n",
    "                    ruler_type: [{\n",
    "                        'rulerID': row['RulerID'],\n",
    "                        'name': name, \n",
    "                        'period': years_field, \n",
    "                        'startYear': year_min,\n",
    "                        'endYear': year_max,\n",
    "                        'searchString': f\"{year_str} {ruler_type} {name} {personalName}\".lower().strip(),\n",
    "                        'personal name': personalName}]}  # Save the ruler as a list\n",
    "            else:\n",
    "                if ruler_type in rulerYears_dict[year_str]:\n",
    "                    rulerYears_dict[year_str][ruler_type].append({\n",
    "                        'rulerID': row['RulerID'],\n",
    "                        'name': name, \n",
    "                        'period': years_field, \n",
    "                        'startYear': year_min,\n",
    "                        'endYear': year_max,\n",
    "                        'searchString': f\"{year_str} {ruler_type} {name} {personalName}\".lower().strip(),\n",
    "                        'personal name': personalName})  # Append to the existing list\n",
    "                else:\n",
    "                    rulerYears_dict[year_str][ruler_type] = [{\n",
    "                        'rulerID': row['RulerID'],\n",
    "                        'name': name, \n",
    "                        'period': years_field,\n",
    "                        'startYear': year_min,\n",
    "                        'endYear': year_max,\n",
    "                        'searchString': f\"{year_str} {ruler_type} {name} {personalName}\".lower().strip(),\n",
    "                        'personal name': personalName}]  # Create a new list\n",
    "\n",
    "    # Export the resulting dictionary to a JSON file\n",
    "    with open('rulersYears.pkl', 'wb') as json_file:\n",
    "        pickle.dump(rulerYears_dict, json_file)\n",
    "    \n",
    "    # Export the resulting dictionary to a JSON file\n",
    "    with open('rulersLists.pkl', 'wb') as json_file:\n",
    "        pickle.dump(rulers_dict, json_file)\n",
    "    \n",
    "    with open('rulersInfo.pkl', 'wb') as json_file:\n",
    "        pickle.dump(myRulers, json_file)\n",
    "# Export the resulting dictionary to a JSON file\n",
    "    with open('rulersYears.json', 'w') as json_file:\n",
    "        json.dump(rulerYears_dict, json_file, indent=4)\n",
    "    \n",
    "    # Export the resulting dictionary to a JSON file\n",
    "    with open('rulersLists.json', 'w') as json_file:\n",
    "        json.dump(rulers_dict, json_file, indent=4)\n",
    "    \n",
    "    with open('rulersInfo.json', 'w') as json_file:\n",
    "        json.dump(myRulers, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to expand the years field (by ChatGPT)\n",
    "def expand_years(years):\n",
    "    if '-' in years:\n",
    "        start_year, end_year = years.split('-')\n",
    "        \n",
    "        # Convert start year\n",
    "        if 'BC' in start_year:\n",
    "            start_year = -int(start_year.replace('BC', ''))\n",
    "        else:\n",
    "            start_year = int(start_year)\n",
    "\n",
    "        # Convert end year\n",
    "        if 'BC' in end_year:\n",
    "            end_year = -int(end_year.replace('BC', ''))\n",
    "        else:\n",
    "            if 'AD' in end_year:\n",
    "                end_year = int(end_year.replace('AD', ''))\n",
    "            elif len(end_year) <= 2:\n",
    "                # Handle short form end year like '95' in '1981-95'\n",
    "                end_year = int(f\"{str(start_year)[:-len(end_year)]}{end_year}\")\n",
    "            else:\n",
    "                end_year = int(end_year)\n",
    "\n",
    "        # Handle wraparounds around centuries\n",
    "        if start_year > end_year:\n",
    "            if start_year > 0 and end_year < 0:\n",
    "                # BC to AD wraparound\n",
    "                return list(range(start_year, 0)) + list(range(1, end_year + 1))\n",
    "            elif start_year < 0 and end_year > 0:\n",
    "                # AD to BC wraparound\n",
    "                return list(range(start_year, 1)) + list(range(-1, end_year - 1, -1))\n",
    "            else:\n",
    "                return list(range(start_year, end_year - 1, -1))\n",
    "        else:\n",
    "            return list(range(start_year, end_year + 1))\n",
    "    else:\n",
    "        if 'BC' in years:\n",
    "            return [-int(years.replace('BC', ''))]\n",
    "        elif 'AD' in years:\n",
    "            return [int(years.replace('AD', ''))]\n",
    "        else:\n",
    "            return [int(years)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcadcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRulersSearch (data,searchFields=['personal name','name']):\n",
    "    rulers_search = {}\n",
    "\n",
    "    for key,row in data.items(): #year level\n",
    "        rulers_search[key] = {}\n",
    "        for title, value in row.items(): #ruler level\n",
    "            for mytitle in value: #list of rulers\n",
    "                titleStringList = []\n",
    "                titleStringList.append (key.casefold())\n",
    "                titleStringList.append (title.casefold())\n",
    "                for key2, value2, in mytitle.items(): #multiple ites per ruler\n",
    "                    if key2 in searchFields and value2:\n",
    "                        titleStringList.append (value2.casefold())\n",
    "                \n",
    "                rulers_search[key][title] = (\" \".join(titleStringList))\n",
    "                    \n",
    "\n",
    "    # Export the resulting dictionary to a JSON file\n",
    "    with open('rulersSearch.json', 'w') as json_file:\n",
    "        json.dump(rulers_search, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "201a414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20': True, '-20': True, '-20-10': True, '-20--10': True, '20-40': True, 'random': False, 'abc-def': False}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_number_like(term):\n",
    "    \"\"\"\n",
    "    Check if the term is a number, BC year, or a range.\n",
    "    Valid formats:\n",
    "    - Single year (e.g., 20 or -20)\n",
    "    - Year range (e.g., -20--10, -20-10, 20-40)\n",
    "    \"\"\"\n",
    "    # Regex for single year or valid range\n",
    "    pattern = r\"^-?\\d+$|^-?\\d+-\\-?\\d+$\"\n",
    "    return bool(re.match(pattern, term))\n",
    "\n",
    "# Sample search terms\n",
    "terms = [\"20\", \"-20\", \"-20-10\", \"-20--10\", \"20-40\", \"random\", \"abc-def\"]\n",
    "\n",
    "# Check if each term is a number or range\n",
    "results = {term: is_number_like(term) for term in terms}\n",
    "\n",
    "# Print results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47a39121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"-20\".isdigit()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
